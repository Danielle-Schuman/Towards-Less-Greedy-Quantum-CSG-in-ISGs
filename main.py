import numpy as np
import random
import pickle
import time
import datetime
import os
import argparse

import subprocess

import utils
from jonas import Jonas
from danielle import Danielle
from n_split_GCSQ import n_split_GCSQ
from r_qubo import r_qubo
from GCSQ import GCSQ
from algorithm import IterativeQuantumAlgorithmWithK
from ours_iterative_exactly import ours_iterative_exactly
from ours_iterative_at_most import ours_iterative_at_most
from k_split_GCSQ_exactly import k_split_GCSQ_exactly
from k_split_GCSQ_at_most import k_split_GCSQ_at_most
from r_qubo_iterative import r_qubo_iterative


# generated by ChatGPT
def create_nested_directory(path):
    try:
        # Check if the path exists
        if not os.path.exists(path):
            # If not, create the missing directories
            os.makedirs(path)
            print(f"Created directory: {path}")
        return True
    except Exception as e:
        print(f"Error creating directory: {e}")
        return False

def create_data_synthetic_test(graph_sizes, num_graphs_per_size, seed, mean=0.5):
    data = {}
    for n in graph_sizes:
        graphs = []
        for graph_num in range(num_graphs_per_size):
            graph = utils.generate_problem(n, mean=mean)
            graphs.append(graph)
        data[n] = graphs
    utils.append_to_pickle(data, f"data/tests/data_{graph_sizes}_{num_graphs_per_size}_{seed}.pkl")
    return data, True


def run_algorithm(serialized_algorithm, serialized_edges, num_agents, serialized_run_id, serialized_directory_path, graph_num):
    try:
        algorithm = pickle.loads(serialized_algorithm)
        edges = pickle.loads(serialized_edges)
        run_id = pickle.loads(serialized_run_id)
        directory = pickle.loads(serialized_directory_path)
        print(f"                    Running {algorithm.name} for seed {algorithm.seed} for graph_size {num_agents} for graph {graph_num} ...")

        # based on code by Jonas Nüßlein
        start_time = time.time()
        coalitions = algorithm.solve(num_agents, edges)
        end_time = time.time()
        value = np.sum([utils.value(c, edges) for c in coalitions])
        total_time = end_time - start_time
        algorithm.data = (coalitions, value, total_time)

        done = (num_agents, graph_num)
        utils.append_to_pickle(algorithm.data, f"{directory}/data_{algorithm.name}__{algorithm.seed}__{run_id}.pkl")
        utils.append_to_pickle(done, f"{directory}/done_{algorithm.name}__{algorithm.seed}__{run_id}.pkl")
        print(f"                    Coalition structure value for {algorithm.name}: {value}   -   Time: {total_time}")
    except ValueError as e:
        raise ValueError(f"                    Error (probably not enough logical qubits available): {str(e)}") from None
    except np.core._exceptions._ArrayMemoryError as e:
        raise Exception(f"                    Error (probably not enough memory available): {str(e)}") from None
    except Exception as e:
        # for e.g. "No embedding found" exception
        raise Exception(f"{str(e)}") from None


def main(algorithm_list, data, graph_sizes, num_graphs_per_size, experiment, directory):
    run_id = str(datetime.datetime.now().date()) + '_' + str(datetime.datetime.now().time()).replace(':', '-')

    for algorithm in algorithm_list:
        too_large = False  # only used in qaoa (or dwave)
        if isinstance(algorithm, IterativeQuantumAlgorithmWithK):
            agents = graph_sizes
            for num_agents in agents:
                if algorithm.k <= num_agents:
                    print(f"\n\n\nTest for graphsize {num_agents}")
                    this_range = range(num_graphs_per_size)
                    for graph_num in this_range:
                        print(f"\n\n     Graph {graph_num}")
                        graph = data[num_agents][graph_num]
                        if synthetic:
                            edges = graph
                        else:
                            edges = utils.transform(graph)

                        print(f"\n          Running {algorithm.name}...")
                        # the following code (call of subprocess) is partially based on code generated by ChatGPT
                        try:
                            # Run algorithm in a subprocess to avoid a potential SIGKILL of the entire script
                            # due to too much memory usage of this algorithm
                            # Serialize the objects and strings to pass to sub-process
                            serialized_algorithm = pickle.dumps(algorithm)
                            serialized_graph = pickle.dumps(edges)
                            serialized_run_id = pickle.dumps(run_id)
                            serialized_directory = pickle.dumps(directory)
                            # subprocess command
                            command = [
                                "python",
                                "-c",
                                f"from main import run_algorithm; "
                                f"print(run_algorithm({serialized_algorithm}, {serialized_graph}, {num_agents}, {serialized_run_id}, {serialized_directory}, {graph_num}))"
                            ]
                            subprocess.run(command, check=True)
                        except subprocess.CalledProcessError as e:
                            print("                    Error: Running algorithm failed, most likely due to insufficient available memory. Error message: ", e)
                            too_large = True
                            if algorithm.solver == "qaoa":
                                break
                if too_large and algorithm.solver == "qaoa":
                    break
                else:
                    pass
        else:
            for num_agents in graph_sizes:
                print(f"\n\n\nTest for graphsize {num_agents}")
                this_range = range(num_graphs_per_size)
                for graph_num in this_range:
                    print(f"\n\n     Graph {graph_num}")
                    graph = data[num_agents][graph_num]
                    if synthetic:
                        edges = graph
                    else:
                        edges = utils.transform(graph)

                    print(f"\n          Running {algorithm.name}...")
                    # the following code (call of subprocess) is partially based on code generated by ChatGPT
                    try:
                        # Run algorithm in a subprocess to avoid a potential SIGKILL of the entire script
                        # due to too much memory usage of this algorithm
                        # Serialize the objects and strings to pass to sub-process
                        serialized_algorithm = pickle.dumps(algorithm)
                        serialized_graph = pickle.dumps(edges)
                        serialized_run_id = pickle.dumps(run_id)
                        serialized_directory = pickle.dumps(directory)
                        # subprocess command
                        command = [
                            "python",
                            "-c",
                            f"from main import run_algorithm; "
                            f"print(run_algorithm({serialized_algorithm}, {serialized_graph}, {num_agents}, {serialized_run_id}, {serialized_directory}, {graph_num}))"
                        ]
                        subprocess.run(command, check=True)
                    except subprocess.CalledProcessError as e:
                        print("                    Error: Running algorithm failed, most likely due to insufficient available memory. Error message: ", e)
                        too_large = True
                        if (algorithm.solver == "qaoa" or algorithm.solver == "dwave"):
                            break
                if too_large and (algorithm.solver == "qaoa" or algorithm.solver == "dwave"):
                    break
    print(f"Done running tests for {experiment}.")


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--seed", type=int, default=None)
    parser.add_argument("--solver", type=str, default="qbsolv")  # alternatively "qaoa" or "dwave"
    args = parser.parse_args()

    solvers = [args.solver]

    # TODO: Determine sensible number of seeds needed for statistical significance
    if args.seed:
        seeds = [args.seed]
    elif solvers[0] == "qbsolv":
        seeds = [3949468976, 2223921875, 1665964778, 1980386705, 1364436995, 2881126339, 2777013863, 3875681486, 4038098247,
         3748732874]
    elif solvers[0] == "qaoa":
        seeds = [3949468976, 2223921875, 1665964778, 1980386705, 1364436995, 2881126339]
    elif solvers[0] == "dwave":
        seeds = [0]  # Seed not relevant for D-Wave
    else:  # does not happen, but code below was used to create seeds in list of qbsolv
        seeds = [random.randint(0, 2 ** 32 - 1)]

    for seed in seeds:
        # Setting the seed
        random.seed(seed)
        np.random.seed(seed)
        print(f"Seed: {seed}")

        # loading E.ON data
        data, synthetic = pickle.load(open('data/data_new_20samples_4_28.pkl', 'rb')), False
        # alternative: load synthetic data
        # data, synthetic = create_data_synthetic_test([10, 15, 20], 25, seed)
        if synthetic:
            data_name = "synthetic"
        else:
            data_name = "eon_data"

        graph_sizes = list(data.keys())  # is [4,6,8,10,12,14,16,18,20,22,24,26,28] for E.ON data
        num_graphs_per_size = len(data[graph_sizes[0]])  # is 20 for E.ON data
        num_graph_sizes = len(graph_sizes)

        parallel = [True]  # , False] -> Try sequential later (maybe)

        for solver in solvers:
            if solver == "qbsolv":
                k_list = [i for i in range(2, graph_sizes[-1])]
            else:
                k_list = [4, 3, 5, 2]
            if solver == "dwave":
                algorithm_list = [Jonas(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver),
                                  Danielle(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver),
                                  r_qubo(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver)
                                  ]
            else:
                algorithm_list = [Jonas(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver),
                                  Danielle(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver),
                                  n_split_GCSQ(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver),
                                  r_qubo(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver)
                                  ]
            directory = f"results/{data_name}/quantum/{solver}"
            directory_exists = create_nested_directory(directory)
            if directory_exists:
                main(algorithm_list=algorithm_list, data=data, graph_sizes=graph_sizes, num_graphs_per_size=num_graphs_per_size,
                     experiment=f"non-iterative algorithms with {solver}", directory=directory)
            for mode in parallel:
                algorithm_list = [GCSQ(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver, parallel=mode)]
                directory = f"results/{data_name}/quantum/{solver}/{'parallel' if mode else 'sequential'}"
                directory_exists = create_nested_directory(directory)
                if directory_exists:
                    main(algorithm_list=algorithm_list, data=data, graph_sizes=graph_sizes,
                         num_graphs_per_size=num_graphs_per_size, experiment=f"GCS-Q with {solver} in {mode} mode",
                         directory=directory)
                for k in k_list:
                    if k == 2:
                        algorithm_list = [
                            ours_iterative_exactly(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver,
                                                   parallel=mode, k=k),
                            ours_iterative_at_most(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver,
                                                   parallel=mode, k=k),
                            r_qubo_iterative(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver,
                                             parallel=mode, k=k)
                        ]
                    elif solver == "dwave":
                        algorithm_list = [ours_iterative_exactly(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver, parallel=mode, k=k),
                                          ours_iterative_at_most(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver, parallel=mode, k=k),
                                          k_split_GCSQ_exactly(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver, parallel=mode, k=k),
                                          r_qubo_iterative(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver, parallel=mode, k=k)
                                          ]
                    else:
                        algorithm_list = [
                            ours_iterative_exactly(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver,
                                                   parallel=mode, k=k),
                            ours_iterative_at_most(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver,
                                                   parallel=mode, k=k),
                            k_split_GCSQ_exactly(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver,
                                                 parallel=mode, k=k),
                            k_split_GCSQ_at_most(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver,
                                                 parallel=mode, k=k),
                            r_qubo_iterative(seed=seed, num_graph_sizes=num_graph_sizes, solver=solver, parallel=mode,
                                             k=k)
                            ]
                    directory = f"results/{data_name}/quantum/{solver}/{'parallel' if mode else 'sequential'}/k={k}"
                    directory_exists = create_nested_directory(directory)
                    if directory_exists:
                        main(algorithm_list=algorithm_list, data=data, graph_sizes=graph_sizes,
                             num_graphs_per_size=num_graphs_per_size, experiment=f"k-split algorithms with {solver} in {mode} mode for k={k}",
                             directory=directory)

